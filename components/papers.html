<table>
  <tr class="paper-item">
    <td class="paper-img">
      <img src="figures/paper/counthallu.png" alt="paper thumbnail">
    </td>
    <td class="paper-content">
      <a href="#" class="paper-title">Counting Hallucinations in Diffusion Models</a>
      <br>
      <strong>Shuai Fu</strong>, Jian Zhou, Qi Chen, Jing Huang, Huy Anh Nguyen, Xiaohan Liu, Zhixiong Zeng, Lin Ma, Quanshi Zhang, Qi Wu
      <br>
      <span class="highlight">Arxiv 2025</span>
      <br>
       <a href="https://shyfoo.github.io/CountHallu-ProjectPage/">project page</a> / <a href="https://arxiv.org/pdf/2510.13080">paper</a> / <a href="https://github.com/ShyFoo/CountHallu-Diff">code</a> / <a href="#">datasets</a>
      <p class="sub-text">
        This work aims to take the first step to quantify hallucination phenomenon in diffusion models from the prospective of counting. 
      </p>
    </td>
  </tr>

  <tr class="paper-item">
    <td class="paper-img">
      <img src="figures/paper/gita.jpg" alt="paper thumbnail">
    </td>
    <td class="paper-content">
      <a href="#" class="paper-title">GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning</a>
      <br>
      Yanbin Wei*, <strong>Shuai Fu*</strong>, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James Kwok, Yu Zhang
      <br>
      <span class="highlight">NeurIPS 2024</span>
      <br>
      <a href="https://v-graph.github.io/">project page</a> / <a href="https://arxiv.org/abs/2402.02130">paper</a> / <a href="https://github.com/WEIYanbin1999/GITA">code</a> / <a href="https://huggingface.co/collections/Yanbin99/gvlqa-datasets">datasets</a> 
      <p class="sub-text">
         We first incorporates visual graphs into general graph reasoning and propose the GVLQA dataset, which the first vision-language dataset for general graph reasoning purposes.
      </p>
    </td>
  </tr>

  <tr class="paper-item">
    <td class="paper-img">
      <img src="figures/paper/nemesis.png" alt="paper thumbnail">
    </td>
    <td class="paper-content">
      <a href="#" class="paper-title">Nemesis: Normalizing the Soft-Prompt Vectors of Vision-Language Models</a>
      <br>
      <strong>Shuai Fu</strong>, Xiequn Wang, Qiushi Huang, Yu Zhang
      <br>
      <span class="highlight">ICLR 2024</span> (Spotlight)
      <br>
      <a href="https://arxiv.org/abs/2408.13979">paper</a> / <a href="https://github.com/ShyFoo/Nemesis">code</a>
      <p class="sub-text">
         We first uncover the "Low-Norm Effect" during prompt tuning VLMs. To harness this effect, we propose Nemesis to normalize the soft-prompt vectors in VLMs.
      </p>
    </td>
  </tr>

  <tr class="paper-item">
    <td class="paper-img">
      <img src="figures/paper/lapdog.jpg" alt="paper thumbnail">
    </td>
    <td class="paper-content">
      <a href="#" class="paper-title">Learning Retrieval Augmentation for Personalized Dialogue Generation</a>
      <br>
      Qiushi Huang, <strong>Shuai Fu</strong>, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian Tang
      <br>
      <span class="highlight">EMNLP 2023</span>
      <br>
      <a href="https://arxiv.org/abs/2406.18847">paper</a> / <a href="https://github.com/hqsiswiliam/LAPDOG">code</a>
      <p class="sub-text">
         We propose LAPDOG to study the potential of leveraging external knowledge for persona dialogue generation.
    </td>
  </tr>

</table>
